{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Practical"
      ],
      "metadata": {
        "id": "3yif1pathVoN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Train an AdaBoost Classifier on a sample dataset and print model accuracy."
      ],
      "metadata": {
        "id": "Bsmu5X6ShcJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize AdaBoost with a weak learner (Decision Tree)\n",
        "base_learner = DecisionTreeClassifier(max_depth=1)  # Weak learner (stump)\n",
        "# Use 'estimator' instead of 'base_estimator'\n",
        "adaboost = AdaBoostClassifier(estimator=base_learner, n_estimators=50, learning_rate=1.0, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "adaboost.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = adaboost.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "bEK23esMiGL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. Train an AdaBoost Regressor and evaluate performance using Mean Absolute Error (MAE)."
      ],
      "metadata": {
        "id": "Vjm1oc3piwEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "# Generate synthetic regression data\n",
        "X, y = make_regression(n_samples=1000, n_features=10, noise=0.2, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize AdaBoost Regressor with a Decision Tree as the base estimator\n",
        "base_estimator = DecisionTreeRegressor(max_depth=4)\n",
        "# Use 'estimator' instead of 'base_estimator'\n",
        "adaboost_regressor = AdaBoostRegressor(estimator=base_estimator, n_estimators=50, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "adaboost_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = adaboost_regressor.predict(X_test)\n",
        "\n",
        "# Evaluate performance using Mean Absolute Error\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f\"Mean Absolute Error: {mae:.4f}\")"
      ],
      "metadata": {
        "id": "0apusIz6kPo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.  Train a Gradient Boosting Classifier on the Breast Cancer dataset and print feature importance."
      ],
      "metadata": {
        "id": "-dI5Zv4VkUbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Gradient Boosting Classifier\n",
        "gb_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "gb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = gb_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate performance using accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Print feature importance\n",
        "feature_importance = pd.Series(gb_classifier.feature_importances_, index=data.feature_names)\n",
        "print(\"Feature Importance:\")\n",
        "print(feature_importance.sort_values(ascending=False))\n"
      ],
      "metadata": {
        "id": "zbmr8xVskj_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Train a Gradient Boosting Regressor and evaluate using R-Squared Score."
      ],
      "metadata": {
        "id": "TkTQU1g6k3iG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "# Generate synthetic regression data\n",
        "X, y = make_regression(n_samples=1000, n_features=10, noise=0.2, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Gradient Boosting Regressor\n",
        "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "gb_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = gb_regressor.predict(X_test)\n",
        "\n",
        "# Evaluate performance using R-Squared Score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-Squared Score: {r2:.4f}\")\n"
      ],
      "metadata": {
        "id": "dv29XI-AkHJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "18.  Train an XGBoost Classifier on a dataset and compare accuracy with Gradient Boosting."
      ],
      "metadata": {
        "id": "7B9Eh-FyldYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Gradient Boosting Classifier\n",
        "gb_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "gb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions with Gradient Boosting\n",
        "y_pred_gb = gb_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate performance using accuracy\n",
        "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
        "print(f\"Gradient Boosting Accuracy: {accuracy_gb:.4f}\")\n",
        "\n",
        "# Initialize and train the XGBoost Classifier\n",
        "xgb_classifier = XGBClassifier(n_estimators=100, learning_rate=0.1, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "xgb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions with XGBoost\n",
        "y_pred_xgb = xgb_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate performance using accuracy\n",
        "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
        "print(f\"XGBoost Accuracy: {accuracy_xgb:.4f}\")\n"
      ],
      "metadata": {
        "id": "r_dwysV-layj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. Train a CatBoost Classifier and evaluate using F1-Score."
      ],
      "metadata": {
        "id": "ehkAYnY4mJyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import catboost as cb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,\n",
        "                           n_redundant=5, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the CatBoost Classifier\n",
        "catboost_model = cb.CatBoostClassifier(random_state=42, verbose=0)\n",
        "catboost_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = catboost_model.predict(X_test)\n",
        "\n",
        "# Calculate the F1-score\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print the F1-score and classification report\n",
        "print(\"F1-Score:\", f1)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Example of using F1 score as the evaluation metric during training.\n",
        "catboost_model_f1_metric = cb.CatBoostClassifier(\n",
        "    random_state=42,\n",
        "    eval_metric='F1',\n",
        "    verbose=0,\n",
        ")\n",
        "\n",
        "catboost_model_f1_metric.fit(X_train, y_train)\n",
        "\n",
        "y_pred_f1_metric = catboost_model_f1_metric.predict(X_test)\n",
        "\n",
        "f1_metric_test = f1_score(y_test, y_pred_f1_metric)\n",
        "\n",
        "print(\"\\nF1-Score with F1 as eval metric:\", f1_metric_test)"
      ],
      "metadata": {
        "id": "dT17OqxfQ2lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. Train an XGBoost Regressor and evaluate using Mean Squared Error (MSE)."
      ],
      "metadata": {
        "id": "xmVIEi89olgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "# Generate synthetic regression data\n",
        "X, y = make_regression(n_samples=1000, n_features=10, noise=0.2, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the XGBoost Regressor\n",
        "xgb_regressor = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "xgb_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_xgb = xgb_regressor.predict(X_test)\n",
        "\n",
        "# Evaluate performance using Mean Squared Error (MSE)\n",
        "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
        "print(f\"XGBoost Mean Squared Error: {mse_xgb:.4f}\")\n"
      ],
      "metadata": {
        "id": "XGKP8Na-ot7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Train an AdaBoost Classifier and visualize feature importance."
      ],
      "metadata": {
        "id": "dmK9ZuP1pmSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import numpy as np\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the AdaBoost Classifier\n",
        "base_estimator = DecisionTreeClassifier(max_depth=1)\n",
        "# Use 'estimator' instead of 'base_estimator'\n",
        "adaboost_classifier = AdaBoostClassifier(estimator=base_estimator, n_estimators=50, random_state=42)\n",
        "adaboost_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_ada = adaboost_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate performance using accuracy\n",
        "accuracy_ada = accuracy_score(y_test, y_pred_ada)\n",
        "print(f\"AdaBoost Accuracy: {accuracy_ada:.4f}\")\n",
        "\n",
        "# Visualize feature importance\n",
        "feature_importance = adaboost_classifier.feature_importances_\n",
        "indices = np.argsort(feature_importance)[::-1]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.title(\"Feature Importance in AdaBoost\")\n",
        "plt.bar(range(X.shape[1]), feature_importance[indices], align=\"center\")\n",
        "plt.xticks(range(X.shape[1]), np.array(data.feature_names)[indices], rotation=90)\n",
        "plt.xlabel(\"Feature\")\n",
        "plt.ylabel(\"Importance\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Y-dihhWtqRT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Train a Gradient Boosting Regressor and plot learning curves"
      ],
      "metadata": {
        "id": "e4TZswkYqqiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "# Generate synthetic regression data\n",
        "X, y = make_regression(n_samples=1000, n_features=10, noise=0.2, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Gradient Boosting Regressor\n",
        "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "gb_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_gb = gb_regressor.predict(X_test)\n",
        "\n",
        "# Evaluate performance using Mean Squared Error (MSE)\n",
        "mse_gb = mean_squared_error(y_test, y_pred_gb)\n",
        "print(f\"Gradient Boosting Mean Squared Error: {mse_gb:.4f}\")\n",
        "\n",
        "# Plot learning curves\n",
        "train_errors, test_errors = [], []\n",
        "for m in range(1, len(X_train)):\n",
        "    gb_regressor.fit(X_train[:m], y_train[:m])\n",
        "    y_train_predict = gb_regressor.predict(X_train[:m])\n",
        "    y_test_predict = gb_regressor.predict(X_test)\n",
        "    train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n",
        "    test_errors.append(mean_squared_error(y_test, y_test_predict))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, len(X_train)), train_errors, label=\"Training Error\")\n",
        "plt.plot(range(1, len(X_train)), test_errors, label=\"Testing Error\")\n",
        "plt.xlabel(\"Training Set Size\")\n",
        "plt.ylabel(\"Mean Squared Error\")\n",
        "plt.title(\"Learning Curves for Gradient Boosting Regressor\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qvsKlLbop20D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Train an XGBoost Classifier and visualize feature importance."
      ],
      "metadata": {
        "id": "jbooFUEOrL8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the XGBoost Classifier\n",
        "xgb_classifier = XGBClassifier(n_estimators=100, learning_rate=0.1, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "xgb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_xgb = xgb_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate performance using accuracy\n",
        "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
        "print(f\"XGBoost Accuracy: {accuracy_xgb:.4f}\")\n",
        "\n",
        "# Visualize feature importance\n",
        "feature_importance = xgb_classifier.feature_importances_\n",
        "indices = np.argsort(feature_importance)[::-1]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.title(\"Feature Importance in XGBoost\")\n",
        "plt.bar(range(X.shape[1]), feature_importance[indices], align=\"center\")\n",
        "plt.xticks(range(X.shape[1]), np.array(data.feature_names)[indices], rotation=90)\n",
        "plt.xlabel(\"Feature\")\n",
        "plt.ylabel(\"Importance\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "73MYWVtbq_no"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Train a CatBoost Classifier and plot the confusion matrix."
      ],
      "metadata": {
        "id": "JvXm-9lssFAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade numpy\n",
        "!pip install --upgrade --force-reinstall catboost # This is to ensure the latest CatBoost version is installed with compatibilities.\n",
        "import catboost as cb # Import after upgrading or reinstalling to use the updated library\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ... (rest of your code remains the same)"
      ],
      "metadata": {
        "id": "dDmfK_fwRMkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade numpy\n",
        "!pip install --upgrade --force-reinstall catboost\n",
        "import catboost as cb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,\n",
        "                           n_redundant=5, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the CatBoost Classifier\n",
        "catboost_model = cb.CatBoostClassifier(random_state=42, verbose=0)\n",
        "catboost_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = catboost_model.predict(X_test)\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot the confusion matrix using seaborn\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C0JtEpXdN5pB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Train an AdaBoost Classifier with different numbers of estimators and compare accuracy."
      ],
      "metadata": {
        "id": "VfPCpTHnvUsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate a synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20,\n",
        "                           n_informative=2, n_redundant=10,\n",
        "                           random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the range of estimators to test\n",
        "n_estimators_range = [10, 50, 100, 200, 500]\n",
        "\n",
        "# Store accuracy scores for each number of estimators\n",
        "accuracy_scores = []\n",
        "\n",
        "# Train and evaluate AdaBoost Classifier for each number of estimators\n",
        "for n_estimators in n_estimators_range:\n",
        "    ada_classifier = AdaBoostClassifier(n_estimators=n_estimators, random_state=42)\n",
        "    ada_classifier.fit(X_train, y_train)\n",
        "    y_pred = ada_classifier.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracy_scores.append(accuracy)\n",
        "    print(f\"Number of Estimators: {n_estimators}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Plot the accuracy scores\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(n_estimators_range, accuracy_scores, marker='o')\n",
        "plt.xlabel('Number of Estimators')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('AdaBoost Classifier Accuracy vs. Number of Estimators')\n",
        "plt.xticks(n_estimators_range)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "G3QTUOaJvpgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. Train a Gradient Boosting Classifier and visualize the ROC curve."
      ],
      "metadata": {
        "id": "TT_H-OV8vxgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "# Generate a synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20,\n",
        "                           n_informative=2, n_redundant=10,\n",
        "                           random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Gradient Boosting Classifier\n",
        "gb_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for the positive class\n",
        "y_scores = gb_classifier.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate the ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_scores)\n",
        "\n",
        "# Calculate the AUC (Area Under the Curve)\n",
        "roc_auc = roc_auc_score(y_test, y_scores)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (AUC = {:.2f})'.format(roc_auc))\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Random guessing line\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate (FPR)')\n",
        "plt.ylabel('True Positive Rate (TPR)')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "#Print AUC\n",
        "print(f\"AUC: {roc_auc}\")"
      ],
      "metadata": {
        "id": "qbArcr9ov8E9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Train an XGBoost Regressor and tune the learning rate using GridSearchCV."
      ],
      "metadata": {
        "id": "BH1ZA1YAwBFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.datasets import make_regression\n",
        "import numpy as np\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the XGBoost Regressor\n",
        "xgbr = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [3, 4, 5, 6],\n",
        "    'subsample': [0.7, 0.8, 0.9],\n",
        "    'colsample_bytree': [0.7, 0.8, 0.9]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=xgbr, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', verbose=2, n_jobs=-1)\n",
        "\n",
        "# Fit the GridSearchCV to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and best score\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "print(\"Best score (negative MSE):\", grid_search.best_score_)\n",
        "\n",
        "# Get the best estimator\n",
        "best_xgbr = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "y_pred = best_xgbr.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Test MSE:\", mse)\n",
        "print(\"Test R2:\", r2)\n",
        "\n",
        "# Example of how to tune only the learning rate, while keeping other params fixed\n",
        "# Define a parameter grid with only learning rate\n",
        "learning_rate_grid = {'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3]}\n",
        "\n",
        "# Initialize GridSearchCV focusing only on learning rate, with fixed other parameters.\n",
        "# You can set other hyperparameters as you want.\n",
        "fixed_params = {\n",
        "    'n_estimators': 200,\n",
        "    'max_depth': 4,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'objective': 'reg:squarederror',\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "xgbr_learning_rate_tuning = xgb.XGBRegressor(**fixed_params)\n",
        "\n",
        "grid_search_learning_rate = GridSearchCV(estimator=xgbr_learning_rate_tuning,\n",
        "                                        param_grid=learning_rate_grid,\n",
        "                                        cv=3,\n",
        "                                        scoring='neg_mean_squared_error',\n",
        "                                        verbose=2,\n",
        "                                        n_jobs=-1)\n",
        "\n",
        "grid_search_learning_rate.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best learning rate parameters:\", grid_search_learning_rate.best_params_)\n",
        "print(\"Best learning rate score (negative MSE):\", grid_search_learning_rate.best_score_)\n",
        "best_xgbr_lr = grid_search_learning_rate.best_estimator_\n",
        "\n",
        "y_pred_lr = best_xgbr_lr.predict(X_test)\n",
        "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
        "r2_lr = r2_score(y_test, y_pred_lr)\n",
        "\n",
        "print(\"Test MSE (Learning Rate Tuning):\", mse_lr)\n",
        "print(\"Test R2 (Learning Rate Tuning):\", r2_lr)"
      ],
      "metadata": {
        "id": "97I7lppjwIcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. Train a CatBoost Classifier on an imbalanced dataset and compare performance with class weighting."
      ],
      "metadata": {
        "id": "GEr1hUvewJJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import catboost as cb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# Create an imbalanced dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,\n",
        "                           n_redundant=5, random_state=42, weights=[0.9, 0.1])\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train CatBoost without class weights\n",
        "catboost_no_weights = cb.CatBoostClassifier(random_state=42, verbose=0)\n",
        "catboost_no_weights.fit(X_train, y_train)\n",
        "y_pred_no_weights = catboost_no_weights.predict(X_test)\n",
        "y_proba_no_weights = catboost_no_weights.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"CatBoost without class weights:\")\n",
        "print(classification_report(y_test, y_pred_no_weights))\n",
        "print(\"AUC:\", roc_auc_score(y_test, y_proba_no_weights))\n",
        "\n",
        "# Calculate class weights\n",
        "class_weights_calculated = class_weight.compute_sample_weight(\n",
        "    class_weight='balanced',\n",
        "    y=y_train\n",
        ")\n",
        "\n",
        "# Train CatBoost with sample weights\n",
        "catboost_with_weights = cb.CatBoostClassifier(random_state=42, verbose=0)\n",
        "catboost_with_weights.fit(X_train, y_train, sample_weight=class_weights_calculated)\n",
        "y_pred_with_weights = catboost_with_weights.predict(X_test)\n",
        "y_proba_with_weights = catboost_with_weights.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"\\nCatBoost with sample weights:\")\n",
        "print(classification_report(y_test, y_pred_with_weights))\n",
        "print(\"AUC:\", roc_auc_score(y_test, y_proba_with_weights))\n",
        "\n",
        "# Train CatBoost using class_weights parameter\n",
        "class_weights_parameter = [sum(y_train == 0) / len(y_train), sum(y_train == 1) / len(y_train)]\n",
        "\n",
        "catboost_class_weights_param = cb.CatBoostClassifier(random_state=42, verbose=0, class_weights=class_weights_parameter)\n",
        "catboost_class_weights_param.fit(X_train, y_train)\n",
        "y_pred_class_weights_param = catboost_class_weights_param.predict(X_test)\n",
        "y_proba_class_weights_param = catboost_class_weights_param.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"\\nCatBoost with class weights as a parameter:\")\n",
        "print(classification_report(y_test, y_pred_class_weights_param))\n",
        "print(\"AUC:\", roc_auc_score(y_test, y_proba_class_weights_param))"
      ],
      "metadata": {
        "id": "6kUnYEyYOrd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. Train an AdaBoost Classifier and analyze the effect of different learning rates."
      ],
      "metadata": {
        "id": "dlsN446BwWS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Create a sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,\n",
        "                           n_redundant=5, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the range of learning rates to test\n",
        "learning_rates = [0.01, 0.05, 0.1, 0.5, 1.0, 1.5, 2.0]\n",
        "\n",
        "# Store the training and testing accuracies for each learning rate\n",
        "train_accuracies = []\n",
        "test_accuracies = []\n",
        "\n",
        "# Train and evaluate AdaBoost for each learning rate\n",
        "for lr in learning_rates:\n",
        "    adaboost = AdaBoostClassifier(learning_rate=lr, random_state=42)\n",
        "    adaboost.fit(X_train, y_train)\n",
        "\n",
        "    y_train_pred = adaboost.predict(X_train)\n",
        "    y_test_pred = adaboost.predict(X_test)\n",
        "\n",
        "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    test_accuracies.append(test_accuracy)\n",
        "\n",
        "    print(f\"Learning Rate: {lr}\")\n",
        "    print(classification_report(y_test, y_test_pred))\n",
        "    print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
        "    print(f\"Test Accuracy: {test_accuracy:.4f}\\n\")\n",
        "\n",
        "# Plot the training and testing accuracies\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(learning_rates, train_accuracies, label='Train Accuracy', marker='o')\n",
        "plt.plot(learning_rates, test_accuracies, label='Test Accuracy', marker='o')\n",
        "plt.xlabel('Learning Rate')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('AdaBoost Learning Rate Analysis')\n",
        "plt.xticks(learning_rates)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Find the best learning rate based on test accuracy\n",
        "best_lr_index = np.argmax(test_accuracies)\n",
        "best_lr = learning_rates[best_lr_index]\n",
        "best_test_accuracy = test_accuracies[best_lr_index]\n",
        "\n",
        "print(f\"Best Learning Rate: {best_lr}\")\n",
        "print(f\"Best Test Accuracy: {best_test_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "k4LToQitygft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Train an XGBoost Classifier for multi-class classification and evaluate using log-loss."
      ],
      "metadata": {
        "id": "yVJ9dVztwtEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "# Load dataset\n",
        "digits = load_digits()\n",
        "X, y = digits.data, digits.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Convert data into DMatrix format for XGBoost\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# Define model parameters\n",
        "params = {\n",
        "    'objective': 'multi:softprob',  # Multiclass classification\n",
        "    'num_class': len(np.unique(y)),  # Number of classes\n",
        "    'eval_metric': 'mlogloss',  # Log loss as evaluation metric\n",
        "    'max_depth': 4,\n",
        "    'eta': 0.3,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'seed': 42\n",
        "}\n",
        "\n",
        "# Train the model\n",
        "num_round = 100  # Number of boosting rounds\n",
        "bst = xgb.train(params, dtrain, num_round)\n",
        "\n",
        "# Make predictions\n",
        "probs = bst.predict(dtest)  # Probabilities for each class\n",
        "\n",
        "# Compute Log Loss\n",
        "logloss = log_loss(y_test, probs)\n",
        "print(f'Log Loss: {logloss:.4f}')\n"
      ],
      "metadata": {
        "id": "I8PLQ30yxE4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZVKN5qDoNCIL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}